{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8670d7c8",
   "metadata": {},
   "source": [
    "# Machine Learning 101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61a19fd",
   "metadata": {},
   "source": [
    "- ***Data Science*** is used to gain ***insights and understanding*** of the data.\n",
    "\n",
    "\n",
    "- ***Machine Learning*** is used to produce ***predictions***."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2d4e0c",
   "metadata": {},
   "source": [
    "## What is Machine Learning?\n",
    "\n",
    "*Machine Learning is essentially about teaching computers to learn from data*:\n",
    "\n",
    "- **Machine Learning is the field of study athat gives computers the ability to learn without being explicitly programmed.**\n",
    "\n",
    "- The idea is that there are ***generic algorithm*** that can tell you something interesting about the set of data ***without     having to write any custom code specific to the problem***.\n",
    "\n",
    "- Instead of writing explicit code, you **feed data** to the generic algorithm and it bulds its own logic based on data.\n",
    "\n",
    "- The learning is possible, because the systems **learns based on the properties** of the objects, a.k.a. *features*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d2a322",
   "metadata": {},
   "source": [
    "## Main Components of Machine Learning\n",
    "\n",
    "\n",
    "### 1. Data\n",
    "    \n",
    "- Data can be collected both manually and automatically.\n",
    "- Data could be images, text, sensor recordings, sound samples or tables of data with many variables and many more.\n",
    "    \n",
    "    \n",
    "### 2. Features\n",
    "\n",
    "- Features are often called variables or parameters.\n",
    "- These are essentially the factors for a machine to look at - the properties of the *object* in question.\n",
    "- Choosing meaningful features is very important.\n",
    "    \n",
    "    \n",
    "### 3. Algorithms\n",
    "\n",
    "- Machine Learning is based on general purpose algorithms.\n",
    "- For example, one kind of algorithm is classisfication.\n",
    "- Classification allows us to put data into different groups.\n",
    "- The interesting thing is that the same classification algorithm used to recognize handwritten numbers and could also be         used to classify emails into spam and not-spam.\n",
    "- This is possible because the algorithm is fed different input data, so it comes up with different classification logic.\n",
    "- The choice of the algorithm is made based on the type of problem at hand. e.g. prediction, classification, etc.\n",
    "- The choice of algorithm is important in determining the quality of the final machine learning model.\n",
    "- One very important thing to remeber is that if the data is *crappy*, even the best algorithm won't help.\n",
    "- ***Garbage in, garbage out is what they always say***.\n",
    "- This is why acquiring as much data as possible is very important first step in getting started with machine learning systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e27c92d",
   "metadata": {},
   "source": [
    "## Types of Machine Learning Algorithms\n",
    "\n",
    "\n",
    "### 1. Supervised Learning\n",
    "\n",
    "- In this type of learning, the training data provided as input to the algorithm imcludes the final solutions.\n",
    "- These solutions are called labels or class because algorithm learns by \"looking\" at the examples with correct answers.\n",
    "- The algorithm has a ***supervisor*** or a ***teacher*** who provides it with all the answers first, and the machine uses       these examples to learn one by one.\n",
    "- Categorizing or identifying prblems are calle ***Classification Problems***. Output only takes two values: 0/1, true/false.\n",
    "- Predecting problems are called ***Regression Problems***. Output is a continuous value or a decimal value.\n",
    "- **Type of algorithm we choose depends on the type of output we want.**\n",
    "- Examples of supervised learning algorithms:\n",
    "    - Linear Regression\n",
    "    - Logistic Regression\n",
    "    - Support Vector Machine\n",
    "    - Decision Trees and Random Forests\n",
    "    - k-Nearest Neighbors\n",
    "    - Neural Networks\n",
    "    \n",
    "\n",
    "### 2. Unsupervised Learning\n",
    "\n",
    "- In this type of learning, the data has no labels.\n",
    "- The goal of the algorithm is to find relationships in the data.\n",
    "- This system needs to learn ***without supervision***.\n",
    "- One type of algorithm is ***Clustering***, where the algorithms tries to find some hidden patterns in the data to segment       data or classify data without any predefined classes and based on some unknown features.\n",
    "- Another well-known use case is image-compression. When saving an image, if we set the palette, let's say, to 32 colors,         clustering will find all the *blueish* pixels, calculate the *average blue* and set it for all the blue pixels. This helps     achieve a lower file size.\n",
    "- Examples of Unsupervised Algorithms:\n",
    "    - Clustering: k-Means\n",
    "    - Visualization and Dimensionality reduction\n",
    "    - Principal Component Analysis (PCA), t-distributed\n",
    "    - Stochastic Neighbor Embedding (t-SNE)\n",
    "    - Association rule learning: Apriori\n",
    "\n",
    "\n",
    "### 3. Semi-supervised Learning\n",
    "\n",
    "- This type of learning deals with partially labelled training data, usually a lot if unlabeled data with some labeled data.\n",
    "- Most semi-supervised learning algorithms are a combination of unsupervised and supervised algorithms.\n",
    "- Google photos is a good example of this. In a set of family photos, the unsupervised part of the algorithm automatically       recognizes the photos in which each of the family memebers appears. For example, it can tell that person A appears in picture   1 and 2. After this step, all the systems needs from us is one label for each person and then supervised part of the           algorithm can name everyone in every photo.\n",
    "\n",
    "\n",
    "### 4. Reinforcement Learning\n",
    "\n",
    "- Reinforcement Learning is a special and more advanced category where the learning system or *agent* needs to learn to make     specific decisions. \n",
    "- The agent observes the environment to which it is exposed, it selects and performs actions, and gets ***rewards*** or           ***penalties*** in return.\n",
    "- Its goal is to choose actions which maximize the reward over time.\n",
    "- So, by trial and error, and based on the pat experience, the system or the ***agent*** learns the best strategy called         ***policy***, on its own.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a9d68a",
   "metadata": {},
   "source": [
    "# Machine Learning Algorithms\n",
    "\n",
    "\n",
    "#### *The key is to know about the different possibilities so that you can then go deeper on a need's basis.*\n",
    "\n",
    "\n",
    "## 1. Linear Regression\n",
    "\n",
    "- Probably the most popular ML algorithm.\n",
    "- Joint two points on acoordinate plane is the example of a very simple ML algorithm, *linear regression*.\n",
    "\n",
    "- Linear regression attempts to represent the relationship between one or more independent variables(points on X axis) and a     numeric outcome(value on Y axis) by fitting the equation of a line to the data\n",
    "    - ***Y = a * X + b***\n",
    "\n",
    "- The goal here is to derive optimal values of ***a*** and ***b*** in the equation above, so our estimated value, ***Y*** can     be as close as possible to their correct values. **Note that** we know the actual value of *Y* during the training phase       because we are trying to learn our equation from the labelled examples given in the training data set.\n",
    "- Once our ML model has learned the line of best fit via linear regresion, this can then be used to predict values for new or     unseen data points.\n",
    "\n",
    "- Different techniques can be used to learn linear regression model. The most popular methods is that of *least squares*:\n",
    "\n",
    "    - **Ordinary Least Squares**: The method of least squares calculates the best-fitting line such that the vertical distance       from the each point to the line are minimum. If a point lies on the fitted line exactly, then its vertical distamce from       the line is 0. To be more specific, in ordinary least squares, the overall distance is the sum of the squares of the           vertical distances for all the data points. The idea is to fit a model by minimizing this squared error or distance. \n",
    "\n",
    "- *When we are dealing with only one variable, we call it **Simple Lineaer Regression**. When there are more than one              independent variables, then this type of regression is called **Multiple Linear Regression**\n",
    "- While finding the line of best fit, we can use a *polynomial*, or a curves line instead of a straight line; this is called     ***Polynomial Regression***.\n",
    "\n",
    "\n",
    "## 2. Logistic Regression\n",
    "\n",
    "- Logistic Regression has the same main ide as the linear regression.\n",
    "- The difference is that, this technique is used when the output or dependent variable is binary, meaning the outcome can only have two possible values. E.g. yes/no; true/false; cat/not cat and so on.\\\n",
    "- In this type of regression, the line of the best fit is not a straight line anymore. \n",
    "- The prediction for the final output is transformed using a non-linear S-shaped function called ***logistic function, g()***.\n",
    "- This logistic function map the intermediate outcome values into an outcome variable *Y* with values ranging from 0 to 1.\n",
    "- These 0 to 1 values can then be interpreted as the probability of occurence if *Y*.\n",
    "\n",
    "\n",
    "## 3.Decision Trees\n",
    "\n",
    "- *Decision tree also belongs to the category of the supervised learning algorithms*, but they can be used for solving both       regression and classification tasks.\n",
    "- In this algorithm, the training model learns to predict values of the target variable by learning ***decision rules*** with a   tree representation. \n",
    "- A tree is made up of nodes corresponding to a *feature or attribute*.\n",
    "- At each node we ask a question about data based on the available features, e.g. *Is it raining or not raining?*. \n",
    "- The left and right branches represent possible answers.\n",
    "- The final nodes, ***leaf nodes***, corresponds to a *class label/predictive value*.\n",
    "- The importance of each feature is determined in a top down approach - **the higher the node, the more important its             attribute/feature**.\n",
    "\n",
    "\n",
    "## 4. Naive Bayes\n",
    "\n",
    "- Maive Bayes is a simple yet wideluy used ML algorithm base on the **Bayes Theorem**.\n",
    "- It is called naive because the classifier assumes that the input variables are independent of each other, *quite a strong and   unrealistic assumption for real data!*\n",
    "- To put it simply, the model is composed of two types of probabilities:\n",
    "    - The probability of each class.\n",
    "    - The conditional probability for each class given value of *x*. *x is the predictor variable*.\n",
    "- Naive Bayes classifiers are actually a popular statistical technique of spam e-mail filtering.\n",
    "- It works by correlating the use of tokens, typically words, with spam and non-spam email and then using Bayes' Theorem to       calculate a probability that an email is spam or not-spam.\n",
    "\n",
    "\n",
    "## 5. Support Vector Machine (SVM)\n",
    "\n",
    "- SVM is a supervised learning algorithm used mainly for *classification* problems.\n",
    "- In this algorithm, we plot each data item as a point in n-dimensional space, where *n* is the number of input features.\n",
    "- Based on these *transformations*, SVM finds an optimal boundary, called ***hyperplane***, that best separates the possible     outputs by their *class label*.\n",
    "- In a two dimensional space, this *hyperplane* can be visualized as a line although not necessarily a straight line.\n",
    "- The task of the SVM  algorithm is ti find the coeeficients that provide the best separation of classes by this *hypeplane*.\n",
    "- The distance between the *hyperplane* and the closest point is called ***margin***.\n",
    "- **The optimal hyperplane** is one that has the largest margin that classifies points in such a way that **the distance         between the closest data point from both classes is maximum**.\n",
    "- In simple words, SVM tries to draw tw lines between the data points with the largest margin between them.\n",
    "\n",
    "\n",
    "## 6. K-Nearest Neighbors (KNN)\n",
    "\n",
    "- KNN algorithm is a very simple and popular technique.\n",
    "- It is based on the following idea from real life: ***You are the average of the five people you most associate with!***\n",
    "- KNN classifies an object by searching through the entire training set for the ***k*** most similar instances, the ***k         neighbors***, and assigning a common output variable to all those *k* instances.\n",
    "- The selection of the *k* value is critical here; a small value can result in a lot of noise and inaccurate results, while a     large value is not feasible and defeats the purpose of the algorithm.\n",
    "- Although mostly used for classification, this technique can also be used for regression problems.\n",
    "- The distance functions for assessing similarity between instances can be *Euclidean, Manhattan, or Minkowski* distance.\n",
    "- ***Euclidean distance***, the most commonly used one, is simply an ordinary straight-line distance between two points.\n",
    "- To be specific, it is the square roo of the sum of the squares of the differences between the cordinates of the points.\n",
    "\n",
    "\n",
    "## 7. K-Means\n",
    "\n",
    "- K-means is a type of *unsupervidsed algorithm* for *data clustering*.\n",
    "- It follows a simple procedure to classify a given data set.\n",
    "- It tries to find ***K*** number of clusters or groups in the dataset.\n",
    "- Since we are dealing with unsupervised learning, all we have is our training data ***X*** and the number of clusters *K*,       that we want to identify, but no labelled training instances (i'e' no data with known final output category that we could use   to train our model).\n",
    "- The algorithm iteratively assigns each data point to one of the *K* groups based on their features.\n",
    "- Initially, it picks *k* points fro each of the K0clusters, known as ***centroid***.\n",
    "- A new data point is put into the cluster having the closest centroid based on the feature similarity.\n",
    "- As new elements are added to the cluster, the cluster centroid is re-computed and keeps changing.\n",
    "- The new centroid becomes the average location of all the data points currently in the cluster.\n",
    "- This process is continued iteratively until the centroids stop changing.\n",
    "- At the end, each centroid is a collection of feature values that define the resulting group.\n",
    "\n",
    "\n",
    "## 8. Random Forest\n",
    "\n",
    "- Random Forest is one of the most popular and powerful machine learning algorithms.\n",
    "- It is a type of ensemble algorithm.\n",
    "- The underlying idea for **ensemble learining** is ***wisdom of crowds***, the idea that the **collective opinion of many is     more likely to be accurate than that of one.***\n",
    "- The outcomne of each of the models is combined and a predictiois made.\\\n",
    "- In Random Forest, we have an ensemble of decision trees, seen earlier in algorithm 3.\n",
    "- When we want to classify a new object, we take the vote of each decision tree and combine the outcome to make a final           decision; majority vote wins.\n",
    "\n",
    "\n",
    "## 9. Dimensionality Reduction\n",
    "\n",
    "- In the last years, there has been an exponential increase in the amount of data captured.\n",
    "- This means that many ML problems involve thousands or even millions of features for each training instance!\n",
    "- This not only makes training extremely slow but also makes finding good solutions much harder.\n",
    "- Thuis problem is often refered as the **curse of dimensionality**.\n",
    "- In real-world problems, it is often possible to reduce the number of features considerably, making problems tractable.\n",
    "- In simple terms, dimnesionality reduction is about assembling specific features into more high-level ones without losing the   most impiortant information. \n",
    "- ***Principal Component Analysis (PCA)*** is the most popular dimensionality reduction technique.\n",
    "- Geomentrically speaking, PCA reduces the dimension of the dataset by squashing it into lower-dimensional line, or more         generally a *hyperplane/subspace*, which retains as much of the original data's salient characteristics as possible.\n",
    "\n",
    "\n",
    "## 10. Artificial Neural Networks (ANN)\n",
    "\n",
    "- ANN are ideal for tackling large and highly complex machine learning tasks, such as recommending the best videos to watch to   hudreds of millions of users every day; powering speech recognition services(e.g. Siri, Google Assistant); or learning to       beat the world ahampion in the game of **Go** (Deepminds AlphaGo).\n",
    "- ANN reuires a ***huge amount of training data, high computational power and long training time*** but, in the end, they are     able to make ***very accurate predictions***.\n",
    "- The key idea behind ANN is to use the brain's architecture for inspiration on how to build intelligent machines.\n",
    "- To train neural network, a set of neurons are mapped out and assigned a random weight which determines how the neurons         process new data, images, texts, sounds, etc.\n",
    "- The correct relationship between inputs and outputs is learned from training the neural network on the input data.\n",
    "- Since during the training phase the systems gets to see the correct answers, it the network doesn't accurately identify the     input, then the system adjust the ***weights***.\n",
    "- Eventually, after sufficient training, the neural network will consistently recognize the correct patterns, e.g.Images.\n",
    "- As neural network is essentially a set of interconnected layers with weighted edges and nodes called **Neurons**.\n",
    "- Between the input and output layers we can insert multiple **hidden layers**.\n",
    "- ANN make use of only two hidden layers.\n",
    "- However, if we increase the depth of these layers then we are ealing with the famous ***Deep Learning***."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d983970a",
   "metadata": {},
   "source": [
    "# Evaluating a ML model\n",
    "\n",
    "\n",
    "## Precision , Recall, and Confusion Matrix\n",
    "\n",
    "\n",
    "- We have learned about various ML models, but how do we evaluate them?\n",
    "- For regression, we can use the difference between the actual and the predicted values.\n",
    "    - *Root Mean Square Error(RSME) or Ordinary least square method, to be more precise*.\n",
    "    - What about classification models?\n",
    "- One might think that accuracy is a good enough measure to evaluate the goodness of a model.\n",
    "- Accuracy is a very important evaluation measure, *but it might not be the nest metric all the time*.\n",
    "\n",
    "### The Accuracy Trap\n",
    "\n",
    "- Say we are building a model that predictsif patients have a chronic illness.\n",
    "- We know that only 0.5% of the patients have the disease, or are '*Positive *' cases.\n",
    "- Now, a dummy model could always give '*Negative*' as a default result and still have high accuracy (99.5%).\n",
    "- Because our dataset is skewed. Out of all patiets, only 0.5% have the disease, so by giving \"Negative\" as a defaultanswer for   100% of the cases, the model is still able to get the prediction right in 99.5% of the cases.\n",
    "- We have a model with very high accuracy! But is this of any good? **Absolutely Not**! \n",
    "- Some important termms:\n",
    "    1. **TP/True Positive**: the case was positive and was predicted positive.\n",
    "    2. **TN/True Negative**: the case was negative and was predicted negative.\n",
    "    3. **FN/False Negative**: the case was positive but was predicted negative. ***Type-2 Error***\n",
    "    4. **FP/False Positive**: the case was negative but was predicted positive. ***Type-1 Error***\n",
    "\n",
    "### Precision, Recall, and Confusion Matrix\n",
    "\n",
    "\n",
    "- Now that we know the meaning of false positives, false negatives, true positives, and true negatives, we can learn about the   famous ***Confusion Matrix***.\n",
    "- A *confusion matrix* has two rows and two columns that report the number of alse positives, false negatives, true positives,   and true negatives.\n",
    "- Basically, it is a summary table showing how good our model is at predicting examples of various classes.\n",
    "- Based on this, we can obtain important measures:\n",
    "    - **Precision**: The ratio of correct positive predictions to a total ***predicted positives***, the ***positive                 predictive value***\n",
    "        - *Precision* = TP / (TP + FP)\n",
    "    - **Recall**: The ratio of correct positive predictions to the ***total actual positives*** examples in the dataset, the         ***Sensitivity***.\n",
    "        - *Recall* = TP / (TP + FN)\n",
    "    - ***F1 Score***: Harmonic mean of *Precision* and *Recall*. *2/(1/Precsion)+(1/Recall)*\n",
    "- Putting this all together, which would be a correct measure to answer the following questions:\n",
    "    1. ***What percentage of our predictions were correct?***\n",
    "        - Accuracy\n",
    "    2. ***What percentage of the positive cases did we identify?***\n",
    "        - Recall\n",
    "    3. ***What percentage of the positive predictions were correct?***\n",
    "        - Precision\n",
    "- In our case of predicting if a person has a chronic ilness, it would be better to have a high *Recall* because we don't want   to leave patients who have the diseaser untreated.\n",
    "- It's better to have false alarms rather than missing positive cases.\n",
    "- So we might be okay with the **low precision but high recall trade-off**.\n",
    "- **Note:** In case dataset is not skewed, but rather balances representaiton of the two classes, then it is okay to use         **Accuracy** as an evaluation measure.\n",
    "\n",
    "\n",
    "\n",
    "## AUC-ROC Curve\n",
    "\n",
    "\n",
    "- Area Under Curve(AUC)-Receiver Operating Characteristics (ROC) Curve is a *prformance measurement* for a classificaion model   at various classification threshold settings.\n",
    "- Basically, it is a probability curve that tells us how well the model is capable of ditinguishing between classes.\n",
    "- The higher the AUC value of our probability curve, th better the model is at predicting 0s as 0s and 1s as 1s.\n",
    "- The ROC curve is plotted with **True positive Rate(*TPR/Recall/Sensitivity*)** against the **False Positive Rate (*FPR/ 1-     specificity*)**, where *TPR is on y-axis and FPR is on x-axis*, where:\n",
    "    - **Sensivity, Recall, Hit Rate, or TPR**\n",
    "        - *Sensivity =  = TP/P = TP/(TP + FN)*\n",
    "    - **Fall-out, (1-specificity), or FPR**\n",
    "        - *1 - Specificity = FPR = FP/N = FP/(FP + TN)*\n",
    "- A great model has *AUC near the 1* indicating it has an excellent measure of separability.\n",
    "- On the other hand, a poor model has AUC near to the 0 meaning it is predicting 0s as 1s and 1s as 0s.\n",
    "- And when AUC is 0.5, it means the models has no class separation capacity and essentially making random predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a486f5ec",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
